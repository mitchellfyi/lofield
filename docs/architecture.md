# Architecture Overview

This document outlines the high-level technical architecture of Lofield FM's AI radio pipeline.

## System Overview

Lofield FM is an AI-powered radio station that generates content in real-time using a combination of large language models (LLMs), text-to-music AI, text-to-speech (TTS) engines, and custom scheduling logic.

The system operates continuously, generating music, presenter commentary, and station imaging to create a 24/7 listening experience.

## Core Components

### 1. Music Generation Pipeline

**Listener Request Flow**:
1. User submits a music request (e.g., "rainy day coding vibes")
2. Request enters the **moderation queue**
3. Community upvotes and moderates requests
4. Approved requests are classified and normalized

**LLM Normalization**:
- Raw user requests are processed by an LLM to generate standardized music prompts
- The LLM ensures prompts align with the lofi aesthetic
- Output is a structured prompt suitable for the text-to-music model
- Handles edge cases (inappropriate requests, off-genre suggestions)

**Text-to-Music Generation**:
- Normalized prompts are sent to a text-to-music AI model
- Model generates lofi instrumental tracks (typically 3-4 minutes)
- Tracks are tagged with metadata (requester, original prompt, generation timestamp)
- Generated audio is queued for playout

**Example Flow**:
```
User: "coffee shop rain sounds"
  ↓
LLM: "Lofi hip-hop with gentle rain ambience, warm jazz chords, soft vinyl crackle"
  ↓
Music AI: [Generates 3:30 lofi track]
  ↓
Queue: Track ready for playout
```

---

### 2. Presenter Commentary System

**AI DJ Generation**:
- Presenter scripts are generated by an LLM following the style guide
- Scripts reference the current show, time of day, and recently played tracks
- The LLM has access to:
  - Show schedule and mood
  - Town bible (Lofield landmarks and running jokes)
  - Current listener requests
  - Seasonal context (date, holidays, weather data)

**Text-to-Speech (TTS)**:
- Generated scripts are converted to speech using TTS
- Each presenter duo has distinct voice profiles
- TTS output is processed to sound natural and radio-appropriate
- Audio is tagged with timing metadata for playout

**Commentary Types**:
1. **Track introductions** (15-30 seconds): Introduce upcoming tracks and read requester names
2. **Longer segments** (1-2 minutes): Topical chat, Lofield references, mild observational humour
3. **Handovers** (5 minutes): Duo-to-duo transitions between shows
4. **Station idents** (5-10 seconds): Brief station IDs and taglines

**Example Generation Flow**:
```
Context: Show = "Deep Work (According to Calendar Blocks)", Time = 10:23, Next track = "Rainy Coding Session"
  ↓
LLM: "That was 'Deep Work Mode' for anyone who's been staring at the same line of code for twenty minutes. Up next, 'Rainy Coding Session' requested by Alex in Bristol. Alex, we hope the syntax is making more sense now than it was earlier."
  ↓
TTS: [Generates audio of presenter saying the script]
  ↓
Queue: Commentary ready to play before next track
```

---

### 3. Request System

**Submission**:
- Users submit song ideas via web interface or API
- Requests include a description/prompt and optional context

**Moderation**:
- New requests enter a moderation queue
- Automated filtering catches obvious spam/inappropriate content
- Community moderation (upvoting/flagging) helps surface quality requests
- Moderators can approve, reject, or request edits

**Classification**:
- Approved requests are classified by mood, time-of-day suitability, and theme
- Classification helps with scheduling (e.g., upbeat requests for morning, chill requests for late night)

**Upvoting**:
- Listeners can upvote requests in the queue
- Higher upvotes increase priority for generation
- Popular themes inform future prompt engineering

**Lifecycle**:
```
Submission → Moderation → Approval → Classification → Queue → Generation → Playout → Archive
```

---

### 4. Playout Engine

The playout engine is the core scheduling system that maintains the 24/7 broadcast.

**Responsibilities**:
- Maintain the 3-hour show schedule
- Ensure correct music-to-talk ratio for each show (40-60% talk, max 60% music)
- Queue music tracks and presenter segments in the correct order
- Handle handover segments between shows
- Insert station idents at appropriate intervals

**Scheduling Logic**:
- Pre-generate a buffer of upcoming content (e.g., 30-60 minutes ahead)
- Match track mood/energy to current show
- Space out presenter commentary appropriately
- Ensure smooth transitions between tracks

**Real-Time Adjustments**:
- If generation fails, fall back to previously generated content
- Adjust timing if tracks run longer/shorter than expected
- Handle live request priority changes

**Output**:
- Continuous audio stream
- Metadata for "now playing" displays
- Timestamps for archiving

---

### 5. Seasonal and Holiday Behavior

**Calendar Awareness**:
- The system has access to current date/time and a calendar of holidays
- LLM prompts are adjusted to reference seasonal context
- Show mood can shift slightly (e.g., darker evenings in winter)

**Seasonal Adaptations**:
- **Weather**: If weather data is available, reference it in presenter scripts
- **Holidays**: Acknowledge major holidays in a low-key way (see style guide)
- **Daylight**: Reference sunrise/sunset times, especially in early morning and evening shows
- **Events**: One-off events (e.g., "it's the summer solstice") mentioned briefly

**Implementation**:
- Seasonal context is passed to the LLM as part of the prompt
- No separate "Christmas mode"—just context-aware adjustments

---

### 6. Archiving and Rewind

**Archiving**:
- All generated content (music and commentary) is archived with metadata
- Archives include:
  - Audio files
  - Timestamps
  - Show information
  - Track metadata (requester, prompt, generation details)

**Rewind Feature**:
- Listeners can access past broadcasts
- Search by date, show, or track
- Useful for finding a specific track or revisiting a segment

**Storage**:
- Compressed audio for efficiency
- Indexed by time and metadata for quick retrieval
- Old archives can be pruned based on retention policy

---

## Data Flow Diagram

```
┌─────────────────┐
│ Listener Request│
└────────┬────────┘
         │
         ▼
┌─────────────────┐     ┌──────────────┐
│ Moderation Queue│────▶│ LLM (Prompt  │
└────────┬────────┘     │ Normalization│
         │              └──────┬───────┘
         │                     │
         ▼                     ▼
┌─────────────────┐     ┌──────────────┐
│  Classification │     │ Text-to-Music│
└────────┬────────┘     │   AI Model   │
         │              └──────┬───────┘
         │                     │
         ▼                     ▼
┌─────────────────────────────────┐
│         Playout Queue           │
│   (Music + Presenter Segments)  │
└────────┬────────────────────────┘
         │
         ▼
┌─────────────────┐     ┌──────────────┐
│  Playout Engine │────▶│  Archive +   │
│                 │     │    Rewind    │
└────────┬────────┘     └──────────────┘
         │
         ▼
┌─────────────────┐
│  Audio Stream   │
└─────────────────┘
```

## Technology Considerations

### LLM (Prompt Normalization and Commentary)

**Requirements**:
- Consistent tone and style adherence
- Fast inference for real-time generation
- Context window large enough for style guide, town bible, and show schedule
- Ability to reference current time, date, and recent tracks

**Potential Models**:
- GPT-4 / GPT-3.5 (OpenAI API)
- Claude (Anthropic API)
- Open-source alternatives (Llama, Mistral) hosted locally

### Text-to-Music AI

**Requirements**:
- Generate lofi music from text prompts
- Output quality suitable for radio broadcast
- Reasonable generation time (under 60 seconds for a 3-minute track)

**Potential Models**:
- MusicGen (Meta)
- Stable Audio (Stability AI)
- Custom fine-tuned model trained specifically on lofi

### Text-to-Speech (TTS)

**Requirements**:
- Natural-sounding voices
- Multiple voice profiles for different presenters
- Consistent quality and tone
- Low latency for real-time generation

**Potential Solutions**:
- ElevenLabs (high quality, multiple voices)
- Google Cloud TTS / Amazon Polly (reliable, scalable)
- Open-source TTS (Coqui, Piper) for cost-effectiveness

### Playout and Streaming

**Requirements**:
- Continuous audio stream with no gaps
- Metadata support for "now playing"
- Reliable failover if generation lags

**Potential Stack**:
- Liquidsoap (radio automation software)
- Icecast (streaming server)
- Custom scheduling logic in Python/Node.js

---

## Scalability and Reliability

### Challenges

1. **Generation Latency**: AI models take time to generate content
2. **API Costs**: LLM and TTS APIs can be expensive at scale
3. **Content Quality**: AI-generated content needs quality control
4. **Continuous Operation**: 24/7 uptime requires robust error handling

### Solutions

1. **Pre-Generation**: Buffer content ahead of time (30-60 minutes)
2. **Caching**: Re-use generated tracks and segments when appropriate
3. **Fallback Content**: Keep a library of pre-generated content for emergencies
4. **Monitoring**: Real-time monitoring of queue depth and generation success rates
5. **Cost Optimization**: Use cheaper models where quality impact is minimal

---

## Future Enhancements

Potential additions to the architecture:

- **Live Listener Interaction**: Real-time chat or reactions
- **Adaptive Scheduling**: AI adjusts show schedule based on listener patterns
- **Voice Cloning**: Train custom voices for more distinct presenter personalities
- **Multi-Language Support**: Generate content in languages other than English
- **Community-Generated Content**: Allow users to submit voice segments
- **Enhanced Personalization**: Listener-specific streams based on preferences

---

## Security and Moderation

### Content Safety

- All user-generated requests are moderated before generation
- LLM prompts include content policy guidelines
- Automated filters catch inappropriate submissions
- Human moderators review edge cases

### Infrastructure Security

- API keys and credentials stored securely
- Rate limiting on request submissions
- DDoS protection for streaming endpoints
- Regular security audits

---

This architecture is designed to be modular and extensible. Components can be swapped out (e.g., different LLM providers, different music generation models) without requiring a full system redesign.

*The goal is reliable, automated, 24/7 AI radio that feels human enough to be companionable but artificial enough to be honest about what it is.*
